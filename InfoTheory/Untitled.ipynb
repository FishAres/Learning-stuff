{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file C:\\Users\\ares\\.julia\\compiled\\v1.0\\PyPlot\\oatAj.ji for PyPlot [d330b81b-6aea-500a-939a-2ce795aea3ee]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    }
   ],
   "source": [
    "using Statistics, PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is information?\n",
    "\n",
    "Bits measure the distinction between two possibilities \n",
    "\n",
    "Bit flips are NOT gates\n",
    "\n",
    "Anything that has 2 states carries 1 bit of information, 4 states -> 2 bits, 8 states -> 3 bits, etc.: $2^n$ possible states. Thus \n",
    "\n",
    "$$ no \\ bits = \\log n $$\n",
    "\n",
    "\n",
    "Universe has ~ $10^{90}$ elementary particles, corresponding to ~ $10^{120}$ bits\n",
    "\n",
    "\n",
    "$10^{90}$ ~= $2^{300}$ => You could use a barcode with 300 bits to barcode every elementary particle in the universe! \n",
    "\n",
    "In contrast we need ~ 43 billion bits to completely describe the human genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolph Landauer: ***\"Information is Physical\"***\n",
    "\n",
    "Both interpretations are accurate:\n",
    "\n",
    "* All bits are physical systems (switches, photon detection etc.)\n",
    "\n",
    "* All physical systems contain information\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy was originally framed in terms of useful work you could extract from a system. For example, by moving a piston to expand a chamber of hot gas, you could get work from the dissipation in kinetic energy of the gas molecules. But you couldn't get *all* of that energy out, due to a certain amount of \"disorder\" in the gas (?). \n",
    "\n",
    "* At the limit, is this tied to the Uncertainty principle?\n",
    "\n",
    "If you have N particles, the entropy S is $S = k \\log N $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information and Probability \n",
    "\n",
    "A priori probabilities\n",
    "\n",
    "Frequencies at the limit of no. repetitions -> infinity\n",
    "\n",
    "Expected # heads = $ \\frac{m}{2} \\pm \\frac{1}{2} \\sqrt{m} $, where m is the number of trials.\n",
    "\n",
    "We can derive this from looking at all possible sequences for *m* repetitions. We count the number of possible sequences with exactly $m_h$ heads and $m_t$ tails, and we can thus relate the sequences to probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {m \\choose m_h} = {m \\choose m_t }  = \\frac{m!}{m_h!(m-m_h)!} $$\n",
    "\n",
    "$$ = \\frac{m!}{m_h!m_t!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the probability of heads or tails with\n",
    "\n",
    "$$ q(h) = \\frac{no. \\ heads}{m} $$\n",
    "$$ q(t) = 1 - q(h) $$\n",
    "\n",
    "The entopy or information for the system is \n",
    "\n",
    "$$ S = -q(h)\\log_2 q(h) - q(t) \\log_2 q(t) $$\n",
    "\n",
    "Which reduces to\n",
    "\n",
    "$$ S = -0.5\\log_2 0.5 -0.5\\log_2 0.5 = $$\n",
    "$$ = -\\log_2 0.5 = -(-1) = 1 \\text{  bit} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log_2 {m \\choose m_h} = mS $$\n",
    "\n",
    "I.e. the number of bits ($\\log_2)$ of the number of possible patterns of heads ${m \\choose m_h}$ is just the number of repetitions m times the information in reach repetition *S* .\n",
    "\n",
    "In general, when we have k possible outcomes:\n",
    "\n",
    "$$ S = -\\sum_{i=i}^{k} q(i)\\log_2 q(i) $$\n",
    "\n",
    "Number of sequences = $2^{mS}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation and logic\n",
    "\n",
    "Bool theorized that any possible logical expression can be written by composing the elementary operations of Boolean logic (X AND Y, NOT C, etc.)\n",
    "\n",
    "Any set of binary operations can be expressed with AND, OR, NOT and COPY gates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information\n",
    "\n",
    "Joint probability p(X,Y) = p(X AND Y)\n",
    "\n",
    "P(X) = P(X, Y) + P(X, NOT Y) (marginal)\n",
    "\n",
    "Logical AND is equivalent to mathematical intersection, OR is equivalent to union\n",
    "\n",
    "Joint information I(X, Y) = $-\\sum_{ij} p(X_i, Y_j) \\log_2 p(X_i, Y_j) $\n",
    "\n",
    "\n",
    "Marginal information I(X) = $-\\sum_i p(X_i)\\log_2 p(X_i) $\n",
    "\n",
    "\n",
    "Mutual information I(X:Y) = I(x) + I(Y) - I(X,Y) or \n",
    "\n",
    "$$ I(X:Y) = -\\sum log_2 \\frac{p(X)p(Y)}{p(X,Y)} $$\n",
    "\n",
    "always >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
